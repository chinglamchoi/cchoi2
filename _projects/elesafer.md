---
title: "Elesafer: Security With Machine Learning"
collection: projects
type: "Computer Vision project"
permalink: /projects/elesafer
venue: ""
date: 2018-07-30
location: "Hong Kong"
---
![YOLOv3](https://chinglamchoi.github.io/cchoi/files/yolov3.png)

We introduce Elesafer -- a Machine Learning 3-in-1 (recognition, alert, tracking) solution that firstly, recognizes that a weapon is being used to commit a crime; secondly, alerts security by raising an alarm; thirdly, uploads and synchronizes details about the suspect (facial features, physique, outfit) via a Cloud database; finally, tracks the suspect real-time via Facial Recognition technology across different cameras of the network, in order to gain information on the offender's escape route. We apply YOLOv3 (You Only Look Once), a real-time object detection model trained on a customized version of the Open Images dataset for detecting armed crimes. We subsequently utilize a Siamese Neural Network for one-shot learning, in order to recognize criminals in the event of a crime. Finally, we outline future development directions, including a mobile application for security management, enabling them to keep track of CCTV footage, Neural Network output, and Cloud information in a user-friendly manner.

YOLOv3 for Multi-class, Multi-label Classification  
======
Although state-of-art models such as Region CNN (R-CNN) and Region-based Fully Convolutional Networks (R-FCN) are unmatched in accuracy, they both perform detection locally using kernels and do not generalize well to latent distributions of data. The You Only Look Once (YOLOv3) architecture is uniquely able to balance speediness in detection and accuracy by reducing chances of overfitting. In Neural Networks such as the R-CNN (also Fast R-CNN, Faster R-CNN), the classification problem is distributed to be carried out across 2 different networks, where one performs classification of image regions for mapping to labels, and the other for pinpointing the location of detected classes. While this region-based approach yields record-breaking levels of accuracy, the complex, two-network approach also implies that it is slow compared to conventional CNN structures. The YOLOv3 algorithm successfully finds a middle-ground between accuracy and speed with the unique property of Unified Detection, by framing the task as a regression problem. 
  
YOLOv3 consists of only 1 single CNN to simultaneously compute bounding detection boxes and probabilities of images corresponding to labels. It does so by considering each image as being overlayed by a grid of $S \times S$ cells. Each output can be represented by a column vector $y$, consisting of the outputted probability that the detected image corresponds to a class, $(x,y)$ coordinates of the mid-point and the height and width of the detection box. Thus each object prediction is assigned to a grid cell-anchor box pair. The shapes of bounding boxes is conventionally found via the K-means algorithm. Although anchor boxes allow multiple objects to be detected in each cell and for bounding boxes to be highly specialized, the number of objects detectable is still limited by the number of anchor boxes. Also, if there are multiple objects in the same grid cell with similar anchor box shapes, YOLO does not accurately predict all occurrences of objects.


Experiments with YOLOv3  
======
In order to quantify how well bounding boxes localize objects, a metric called Intersection Over Union (IoU) is applied, defined as $\frac{\textrm{size of intersection}}{\textrm{size of union}}$ between the bounding box predicted and the ground truth. In implementation, a threshold value is chosen depending on the desired strictness towards the output. For instance, if this threshold is defined as being 0.5, only predictions with $IoU \geq 0.5$ are recognized as correct classifications. Intuitively, the higher the $IoU$ of each prediction, the more accurate the bounding box is. YOLO further optimizes its detection process using a method called Non-max Suppression, to avoid repeated detections of the same object. It does so by examining the probabilities of detection and assigning a confidence level $C$ to each prediction. Using the following definition of confidence, this function outputs only the highest confidence bounding box, while others with a high $IoU$ but a lower confidence are suppressed.
  
This successfully eliminates boxes which overlap significantly with the designated output box, thus helping prevent outputting multiple bounding boxes for the same object. The above considerations optimize the YOLO algorithm for computing accurate bounding boxes in terms of both shape and location, which is not satisfied with the alternate sliding windows detection approach. Apart from its ability to process an impressive number of image frames per second, YOLOv3 is also distinguished for its accuracy. In object detection problems, the accuracy is detected using the Mean Average Precision (mAP) algorithm, defined as
![mAP](https://chinglamchoi.github.io/cchoi/files/map.PNG)  
In order to implement the YOLOv3 algorithm, we used the pre-trained weights file [provided by Joseph Redmon](https://pjreddie.com/media/files/yolov3-openimages.weights), the first author of the YOLO papers. For the purposes of this task, we trained YOLOv3 on images listed in Google's Open Images v4 dataset, which is one of the few free-to-use datasets which contain the required classes of "Knife" (Knife, Kitchen Knife) and "Gun" (Shotgun, Handgun). We made use of the OpenCV library in order to stream footage for detection with YOLOv3. Although we did not rigorously test the accuracy our our implementation of YOLO, we experimented with the model ourselves. We were able to verify that YOLOv3 consistently recognizes weapons such as knives and kitchen knives (as well as other non-weapon classes such as person, backpack, fork), but had trouble testing out its effectiveness in recognizing guns due to legal reasons. We achieved a maximum FPS of 9.36 in our implementation.
    
Facial Recognition with Siamese Neural Network  
======
![Siamese network](https://chinglamchoi.github.io/cchoi/files/siamese.png)  
We implement the Siamese Neural Network (SNN) to facilitate one-shot learning. In Elesafer, when a person is detected to have carried out a crime with a weapon, our pipeline will save images taken of him/her to a database. It will simultaneously activate the suspect recognition component of our system. Upon activation, the CCTV footage of all cameras in the same network will be passed into our SNN. Form the input footage, we aim to verify whether or not the suspect has been spotted, and if so, by which camera at which position. This operation is performed with an SNN, as there is no prior training data of the suspects' facial features or physique to refer to. This is a special class of problems which require one-shot learning capabilities.
  
In terms of implementation, due to insufficient computational resources, we implemented a [pre-trained model of the FaceNet architecture](https://github.com/davidsandberg/facenet). This is a Tensorflow implementation of FaceNet trained on the VGG Face2 dataset and achieved an astounding Labeled Faces in the Wild (LFW) accuracy of 99.65%. It makes use of the Inception ResNet v1 architecture, a Residual blocks based, very deep NN. After running the pre-trained model, we were able to test its performance ourselves. Although we did not conduct formal testing of the model, out of the 80 trials which we performed, the pre-trained implementation of FaceNet was able to obtain an LFW accuracy of 100%. It is evident that the model is near optimal for one-shot learning.

Conclusion
======
In Elesafer, We have designed a smart security camera system which takes advantage of robust Deep Learning models to enhance security. Our system makes use of 2 advanced Computer Vision techniques: You Only Look Once (v3) for the detection of weapons in the input footage; Siamese Neural Network to implement one-shot learning, and to recognize faces of suspects detected to have used a weapon. By automatizing crime detection, our system minimizes the possibility of crimes going undetected due to human factors. While our model is far from complete (applicable in real-life), we are hard at work in optimizing various components of Elesafer. As one of our major inspirations, Professor Fei-Fei Li said, "I believe AI and its benefits have no borders. Whether a breakthrough occurs in Silicon Valley, Beijing, or anywhere else, it has the potential to make everyone's life better for the entire world": we earnestly aspire to this goal.
