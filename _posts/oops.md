---
title: 'OOPS! Predicting Unintentional Action in Video'
date: 2020-06-25
permalink: /posts/2020/06/oops/
tags:
  - computer vision
  - deep learning
---
Understanding the Intentionality of Motion  
![OOOPS!](https://chinglamchoi.github.io/cchoi/files/oops.png)

Article published in Towards Data Science: [https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3](https://towardsdatascience.com/oops-predicting-unintentional-action-in-video-87626aab3da3)   
Realistically, humans are imperfect agents whose actions can be erratic and unpredictable. While prior research focuses largely on human activity recognition and prediction, Columbia University researchers adopt a new approach -- analysing goal-directed human action. Presented in CVPR 2020, Dave Epstein, Boyuan Chen, and Carl Vondrick make valuable contributions in "OOPS! Predicting Unintentional Action in Video" [1]:   
1. Proposal of 3 new tasks: classification, localisation and anticipation of unintentional action  
2. Introduction of a new benchmark dataset: large, public and (partially) annotated (optical flow, timestamps of unintentional motion, etc)  
3. Comparing mid-level perceptual clues of unintentional action: Video speed (new), video context [2], event order [3]  

Intentionality of Motion
======
Numerous research has sought to model the physical and atomic consequences of human action, yet few try to understand the intentions behind motion. This paper distinguishes between intentional and unintentional motion, aiming to identify, localise and predict unintentional motions.  
  
OOPS! Dataset
======
The OOPS! Dataset [4] consists of 20,338 video clips (3-30 seconds long, totalling 50+ hours) from YouTube fail compilations -- all verified to contain some unintentional, "in-the-wild" human action. As the authors propose a self-supervised approach to this task, the dataset is split into 3 subsets: 7,368 videos as the labelled training set, 6,739 labelled videos as the test set, and the rest being the unlabelled set for pre-training. For classification, actions in videos are labelled as "intentional", "unintentional" or "transitional"; for localisation, workers annotate timestamp markers at the temporal locations of failure (the moment when failure starts). Additional dataset annotations include optical flow and Natural Language descriptions. Within the dataset, 270 videos are designated as the diagnostic set, which see more fine-grained manual annotation. Such videos are categorised into 9 types of unintentional action: "Limited skill", "Limited knowledge", "Environmental", "Unexpected", "Limited visibility", "Planning error", "Execution error", "Single-agent", "Multi-agent".  
![Dataset statistics](https://chinglamchoi.github.io/cchoi/files/stats.png)  
The authors also report various dataset statistics, including the distribution of video clip lengths and failure time labels to illustrate the diversity of data; standard deviation of labels from different human annotators to demonstrate high human (annotator) agreement; distribution of action and scene categories (as predicted by their fully-supervised baseline).  

Self-Supervised Features from Mid-Level Perceptual Clues
======
The authors investigate self-supervisory clues which are naturally present in videos (or requires minimal annotation), to learn deep, transferable representations of intentionality in human action. Specifically, video speed, video context and event order are examined. All ConvNets are implemented via the ResNet3D-18 model [5].  

Video Speed
======
Building on prior research [6], the authors point out that human judgement of intentionality is substantially affected by video speed. As video speed is intrinsic to each video, inference by speed requires minimal pre-processing. For training, the authors synthetically alter the speeds of videos, and train a self-supervised ConvNet to predict the true frame-rate. As noted by the authors, the features generated by this ConvNet are correlated to the expected duration of events (robustness derived from training on videos with synthetically altered speeds), and encode frame-by-frame motion information, thus constructing a useful representation of video speed information.  

Video Context
======
Authors state that "unintentional action is often a deviation from expectation", and explore the predictability of frames as a visual metric for intentionality. Guided by prior research [2], they consider frames $x_{t-1}$ and $x_{t+1}$ as surrounding video context, and incentivise the model to interpolate the feature map of the middle target frame $x_t$. Notably, they utilise concepts of noise-contrastive estimation [7] and contrastive predictive coding [2, 3, 8] to structure the objective function to be maximised:
![Objective function](https://chinglamchoi.github.io/cchoi/files/objective.png)  
in order to maximise distance between target frame features and the contextual embedding, while minimising that of the target frame and non-contextual clip features.  

Event Order
======
Authors present the rationale that "unintentional motion often manifests as chaotic or irreversible motion", leading to a distinctive temporal event order. To generate a representation of event order [3], they permutate and shuffle subsampled clips from videos, and train a ConvNet to predict the applied permutation sequence. This is achieved via a 3-part model, consisting of a clip feature encoder, pairwise clip relation network (where features denote the similarity of clips), and event order predictor.
  
Features extracted from the above self-supervised models are then used as input for a linear classifier, which performs 3-class classification with classes: "intentional", "unintentional", and "transitional" motion.
  
Experimentation
======
Performance is benchmarked on 3 tasks -- classification, localisation (localising the temporal boundary of transition from intentional to unintentional motion), anticipation (predicting the onset of failure), and make 3 levels of comparison: Firstly, comparing between different self-supervisory incidental clues (video speed (newly proposed), video context, event order); secondly, comparing self-supervised models and fully-supervised baselines (pre-training on the Kinetics action recognition dataset plus fine-tuning, fine-grained annotations: motion magnitude, Scratch, Chance); lastly, comparing machine and human performance (human agreement).  
![Tables](https://chinglamchoi.github.io/cchoi/files/tables.png)  
  
Classification, Localisation, Anticipation
======
Across all 3 tasks, Kinetics supervision yields the best machine performances, while video-speed supervision consistently outperforms all other self-supervised and fully-supervised methods. The performance gap between fully-supervised and self-supervised methods is least in classification (Table 1), and greatest in temporal localisation (Table 2). To quantify localisation accuracy, predictions which overlap (results reported for within 1s and 0.25s) with any of the ground truth temporal locations (some videos have multiple ground truths) are considered correct.  
![Classification confusion matrices](https://chinglamchoi.github.io/cchoi/files/confusion.png)  
In particular, self-supervised models suffered more false positive boundary predictions than fully-supervised ones, where they conflate intentional motion with the start of failure (Fig. 2). Additionally, the authors conduct detailed error rate analysis (Fig. 3) for each of the 9 unintentional motion categories (in the aforementioned diagnostic set). They report that unintentional motion caused by unexpected ("such as a bird swooping in suddenly") or environmental ("such as slipping on ice") factors are the most difficult to detect, and hypothesise that multi-agent scenes record the lowest error rate due to their more obvious visual clues. Other challenges include limited video visibility (occluded objects) and limited knowledge ("such as understanding that fire is hot"). As seen from the results, both self-supervised and fully-supervised methods lag significantly behind human performance.  
![Performance breakdown](https://chinglamchoi.github.io/cchoi/files/performance.png)  

Conclusion
======
"OOPS! Predicting Unintentional Action in Video" introduces 3 new tasks for understanding intentionality in human actions, and presents a large benchmark dataset for future work. The authors propose a self-supervised approach, and report promising results using video speed as an incidental clue for video representation.  
  
References
======
[1] Dave Epstein, Boyuan Chen, and Carl Vondrick. OOPS! Predicting unintentional action in video. In CVPR, 2020.  
[2] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.  
[3] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In CVPR, 2019.  
[4] Dave Epstein, Boyuan Chen, and Carl Vondrick. OOPS! Predicting Unintentional Action in Video. Retrieved from https://oops.cs.columbia.edu/  
[5] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In CVPR, 2018.  
[6] Eugene M Caruso, Zachary C Burns, and Benjamin A Converse. Slow motion increases perceived intent. In PNAS, 2016.  
[7] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.  
[8] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCV Workshops, 2019.  
