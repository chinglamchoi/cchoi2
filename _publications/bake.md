---
title: "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification"
collection: publications
permalink: /publication/bake
excerpt: '<img src="https://chinglamchoi.github.io/cchoi/files/bake.png" width="60%" class="center"> <br> <style> .center {display: block; margin-left: auto; margin-right: auto; width: 50%;} </style> The recent studies of knowledge distillation have discovered that ensembling the &quot;dark knowledge&quot; from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +1.2% gain of ResNet-50 on ImageNet with only +3.7% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.'
date: 2021-04-27
venue: 'arXiv Preprint'
paperurl: 'https://chinglamchoi.github.io/cchoi/files/bake.pdf'
citation: 'Yixiao Ge, Xiao Zhang, Ching Lam Choi, Peipei Zhao, Feng Zhu, Rui Zhao, Hongsheng Li. Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification. <i>arXiv preprint arXiv:2104.13298,</i> 2021.'
---
![BAKE](https://chinglamchoi.github.io/cchoi/files/bake.png)
The recent studies of knowledge distillation have discovered that ensembling the &quot;dark knowledge&quot; from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +1.2% gain of ResNet-50 on ImageNet with only +3.7% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.

[Download paper here](https://chinglamchoi.github.io/cchoi/files/bake.pdf)

Recommended citation: Yixiao Ge, Xiao Zhang, Ching Lam Choi, Peipei Zhao, Feng Zhu, Rui Zhao, Hongsheng Li. Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification. <i>arXiv preprint arXiv:2104.13298,</i> 2021.
