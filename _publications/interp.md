---
title: "Generalization of Interpretable Deep Learning Requires More Data"
collection: publications
permalink: /publication/interp
excerpt: '<img src="https://chinglamchoi.github.io/cchoi/files/interp.png" width="80%" class="center"> <br> <style> .center {display: block; margin-left: auto; margin-right: auto; width: 50%;} </style> Feature saliency maps are commonly used for interpreting neural network predictions. This approach to interpretability is often studied as a post-processing problem independent of training setups, where the gradients of trained models are used to explain their output predictions. However, in this work, we observe that gradient-based interpretation methods are highly sensitive to the training set: models trained on disjoint datasets without regularization produce inconsistent interpretations across test data. Our numerical observations pose the question of how many training samples are required for accurate gradient-based interpretations. To address this question, we study the generalization aspect of gradient-based explanation schemes and show that the proper generalization of interpretations from training samples to test data requires more training data than standard deep supervised learning problems. We prove generalization error bounds for widely-used gradient-based interpretations, suggesting that the sample complexity of interpretable deep learning is greater than that of standard deep learning. Our bounds also indicate that Gaussian smoothing in the widely-used SmoothGrad method plays the role of a regularization mechanism for reducing the generalization gap. We evaluate our findings on various neural net architectures and datasets, to shed light on how training data affect the generalization of interpretation methods.'
date: 2023-03-08
venue: 'Preprint'
paperurl: 'https://chinglamchoi.github.io/cchoi/files/interp.pdf'
citation: 'Ching Lam Choi, Farzan Farnia. Generalization of Interpretable Deep Learning Requires More Data. 2023.'
---
![UAD](https://chinglamchoi.github.io/cchoi/files/interp.png)
Feature saliency maps are commonly used for interpreting neural network predictions. This approach to interpretability is often studied as a post-processing problem independent of training setups, where the gradients of trained models are used to explain their output predictions. However, in this work, we observe that gradient-based interpretation methods are highly sensitive to the training set: models trained on disjoint datasets without regularization produce inconsistent interpretations across test data. Our numerical observations pose the question of how many training samples are required for accurate gradient-based interpretations. To address this question, we study the generalization aspect of gradient-based explanation schemes and show that the proper generalization of interpretations from training samples to test data requires more training data than standard deep supervised learning problems. We prove generalization error bounds for widely-used gradient-based interpretations, suggesting that the sample complexity of interpretable deep learning is greater than that of standard deep learning. Our bounds also indicate that Gaussian smoothing in the widely-used SmoothGrad method plays the role of a regularization mechanism for reducing the generalization gap. We evaluate our findings on various neural net architectures and datasets, to shed light on how training data affect the generalization of interpretation methods.
[Download paper here](https://chinglamchoi.github.io/cchoi/files/interp.pdf)

Recommended citation: Ching Lam Choi, Farzan Farnia. Generalization of Interpretable Deep Learning Requires More Data. 2023.
